---
title: 'Overt visual attention and value computation in
complex risky choice'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - admin
  - Jacob Elsey
  - Aurelien Wyngaard
  - Youping Yang
  - Aaron Sampson
  - Erik Emeric
  - Moshe Glickman
  - Marius Usher
  - Dino Levy
  - Veit Stuphorn
  - Ernst Niebur

# Author notes (optional)
author_notes:
  - 'Equal contribution'
  - 'Equal contribution'

date: '2013-07-01T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2017-01-01T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['3']

# Publication name and optional abbreviated publication name.
publication: In *bioRxiv*
publication_short: In *bioRxiv*

abstract: Traditional models of decision making under uncertainty explain human behavior in simple situations with a minimal set of alternatives and
attributes. Some of them, such as prospect theory, have been proven successful and robust in such simple situations. Yet, less is known about
the preference formation during decision making in more complex cases.
Furthermore, it is generally accepted that attention plays a role in the
decision process but most theories make simplifying assumptions about
where attention is deployed. In this study, we replace these assumptions
by measuring where humans deploy overt attention, i.e. where they fixate. To assess the influence of task complexity, participants perform two
tasks. The simpler of the two requires participants to choose between
two alternatives with two attributes each (four items to consider). The
more complex one requires a choice between four alternatives with four
attributes each (16 items to consider). We then compare a large set of
model classes, of different levels of complexity, by considering the dynamic
interactions between uncertainty, attention and pairwise comparisons between attribute values. The task of all models is to predict what choices
humans make, using the sequence of observed eye movements for each
participant as input to the model. We find that two models outperform
all others. The first is the two-layer leaky accumulator which predicts human choices on the simpler task better than any other model. We call the
second model, which is introduced in this study, TNPRO. It is modified
from a previous model from management science and designed to deal
with highly complex decision problems. Our results show that this model
performs well in the simpler of our two tasks (second best, after the accumulator model) and best for the complex task. Our results suggest that,
when faced with complex choice problems, people prefer to accumulate
preference based on attention-guided pairwise comparisons.
# Summary. An optional shortened abstract.
summary: Decision Making in complex case
tags: []

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  - []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: example
---

{{% callout note %}}
Click the _Cite_ button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the _Slides_ button to check out the example.
{{% /callout %}}

Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).
